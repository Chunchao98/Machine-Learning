1. batch epoch iteration

----------------------------------------------------------
深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式。

第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，

计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。

另一种，每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，但是收敛性能不太好，

可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。

为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，

这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。

现在用的优化器SGD是stochastic gradient descent的缩写，但不代表是一个样本就更新一回，还是基于mini-batch的。

那 batch epoch iteration代表什么呢？

（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；

（2）iteration：1个iteration等于使用batchsize个样本训练一次；

（3）epoch：1个epoch等于使用训练集中的全部样本训练一次，通俗的讲epoch的值就是整个数据集被轮几次。

比如训练集有500个样本，batchsize = 10 ，那么训练完整个样本集：iteration=50，epoch=1.

batch: 深度学习每一次参数的更新所需要损失函数并不是由一个数据获得的，而是由一组数据加权得到的，这一组数据的数量就是batchsize。

batchsize最大是样本总数N，此时就是Full batch learning；最小是1，即每次只训练一个样本，这就是在线学习（Online Learning）。当我们分批学习时，

每次使用过全部训练数据完成一次Forword运算以及一次BP运算，成为完成了一次epoch。
----------------------------------------------------------------
原文：https://blog.csdn.net/qq_18668137/article/details/80883350 

总结：
深度学习，感觉都是误差反传，这个误差要怎样计算呢？
1.全部样本跑一遍， 然后计算误差 （批梯度下降）
2.每个样本跑完，便计算误差      （随机梯度下降）
3.**广泛采用折中方法，小批的梯度下降。 做法是把所有的数据分成若干批，按批来更新参数
所以上面的 epoch,batchsize,iteration的概念也就是对应第三种方法
epoch:     整个样本集被轮几次
batchsize: 每次训练取batchsize个样本进行训练
iteration: 使用batchsize个样本训练几次



